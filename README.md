<h1 align="center">ETL Pipeline for Data Warehouse</h1>
<p align="center">
  <a href="https://github.com/PoojaKabadi/Cloud-Datawarehouse/tree/main">
    <img src="https://github.com/user-attachments/assets/689ef7bd-33dd-448c-94c5-c4a1d6b008fb" alt="ETL Pipeline Overview" width="900px" />
  </a>
  <br><i>
    Built a robust ETL pipeline using Apache Hop and Oracle SQL
    to automate dimension/fact table loading, supporting core<br> 
    analytical queries and enabling clean data modeling.
  </i><br>
</p>


**📘 Description:**  
This project demonstrates a working ETL pipeline that integrates customer, product, and sales data into a centralized data warehouse. Using **Apache Hop**, I designed four modular pipelines to:
- Load and transform dimension tables (`Dim_Date`, `Dim_Customer`, `Dim_Product`)
- Dynamically update records with “punch-through” logic from Excel inputs
- Stream-join surrogate keys into the `Fact_Sales` table via Apache Hop
- Validate ETL results using **Oracle Cloud SQL**

This hands-on project highlights my ability to build scalable data ingestion systems, model star schemas, and orchestrate multi-source transformations in a structured BI pipeline.

📅 **Timeline**: April–May 2025  
📦 **Tools Used**: Apache Hop, Oracle SQL, Star Schema, Stream Lookup, Excel, CSV  

